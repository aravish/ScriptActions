{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import pandas as pd\n#pd.show_versions()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1541449485506_0021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:8088/proxy/application_1541449485506_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:30060/node/containerlogs/container_1541449485506_0021_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "#ensure you have all the required libraries\n%%sh\n/usr/bin/anaconda/bin/pip list | grep 'azure-datalake-store\\|azure-mgmt-datalake-store\\|azure-mgmt-resource\\|pandas\\|pyarrow'", "outputs": [{"output_type": "stream", "name": "stdout", "text": "azure-datalake-store (0.0.39)\nazure-mgmt-datalake-store (0.5.0)\nazure-mgmt-resource (2.0.0)\npandas (0.23.4)\npyarrow (0.11.1)\n"}, {"output_type": "stream", "name": "stderr", "text": "You are using pip version 8.1.2, however version 18.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# import required ADL libraries\nfrom azure.common.credentials import ServicePrincipalCredentials\nfrom azure.datalake.store import core, lib, multithread", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# setup ADL authentication, get param values from Azure AD\ntoken = lib.auth(tenant_id = '72f988bf-86f1-41af-91ab-2d7cd011db47', client_secret = 'YO00AS0dOPF2u9uQutzUYCoS+loYvr5fD6jEkGANOg4=', client_id = '127fd200-1733-4782-8fa4-94c0c16a3e04')", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "# initialize ADL file system client\nadlsFileSystemClient = core.AzureDLFileSystem(token, store_name='aravishdatalake')\n\n# Read a file into pandas dataframe\n# samplefile = adlsFileSystemClient.open('/aravishfolder/userdata.parquet', 'rb') ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "#read using python ADL libraries, directly from ADLS as pandas data frame\nwith adlsFileSystemClient.open('/aravishfolder/userdata.parquet', 'rb') as f:\n    dfpd = pd.read_parquet(f, engine='pyarrow') \n\n# Show the dataframe\ndfpd.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "    registration_dttm  id   ...                      title comments\n0 2016-02-03 07:55:29   1   ...           Internal Auditor    1E+02\n1 2016-02-03 17:04:03   2   ...              Accountant IV         \n2 2016-02-03 01:09:31   3   ...        Structural Engineer         \n3 2016-02-03 00:36:21   4   ...     Senior Cost Accountant         \n4 2016-02-03 05:05:31   5   ...                                    \n\n[5 rows x 13 columns]"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "#adl file object\nsamplefile", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<ADL file: /aravishfolder/userdata.parquet>"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "#read parquet using spark sql context\ndf = sqlContext.read.parquet('adl://aravishdatalake.azuredatalakestore.net/aravishfolder/userdata.parquet')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- registration_dttm: timestamp (nullable = true)\n |-- id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- cc: string (nullable = true)\n |-- country: string (nullable = true)\n |-- birthdate: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- title: string (nullable = true)\n |-- comments: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "df.rdd.getNumPartitions()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1"}], "metadata": {"collapsed": false}}, {"execution_count": 33, "cell_type": "code", "source": "# Show the spark dataframe\ndf.show(5, True)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\n|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|     country|birthdate|   salary|               title|comments|\n+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\n|2016-02-03 07:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|   Indonesia| 3/8/1971| 49756.53|    Internal Auditor|   1E+02|\n|2016-02-03 17:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |      Canada|1/16/1968|150280.17|       Accountant IV|        |\n|2016-02-03 01:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|      Russia| 2/1/1960|144972.51| Structural Engineer|        |\n|2016-02-03 00:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female| 140.35.109.83|3576031598965625|       China| 4/8/1997| 90263.05|Senior Cost Accou...|        |\n|2016-02-03 05:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      |169.113.235.40|5602256255204850|South Africa|         |     null|                    |        |\n+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "#convert spark data frame to pandas data frame\npddatafram = df.toPandas()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "#check pandas data frame\npddatafram.head()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "    registration_dttm  id   ...                      title comments\n0 2016-02-03 07:55:29   1   ...           Internal Auditor    1E+02\n1 2016-02-03 17:04:03   2   ...              Accountant IV         \n2 2016-02-03 01:09:31   3   ...        Structural Engineer         \n3 2016-02-03 00:36:21   4   ...     Senior Cost Accountant         \n4 2016-02-03 05:05:31   5   ...                                    \n\n[5 rows x 13 columns]"}], "metadata": {"collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "sc._conf.getAll()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(u'spark.ui.enhancement.enabled', u'true'), (u'spark.history.kerberos.keytab', u'none'), (u'spark.eventLog.enabled', u'true'), (u'spark.yarn.containerLauncherMaxThreads', u'25'), (u'spark.driver.extraLibraryPath', u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'), (u'spark.executor.extraJavaOptions', u'-Dhdp.version= -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC'), (u'spark.sql.cbo.joinreorder.enabled', u'true'), (u'spark.app.id', u'application_1541449485506_0021'), (u'spark.repl.class.outputDir', u'/mnt/resource/hadoop/yarn/local/usercache/livy/appcache/application_1541449485506_0021/container_1541449485506_0021_01_000001/tmp/spark6302536671453032091'), (u'spark.executor.extraLibraryPath', u'/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'), (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'), (u'spark.executor.memoryOverhead', u'384'), (u'spark.locality.wait', u'0'), (u'spark.yarn.app.id', u'application_1541449485506_0021'), (u'spark.app.name', u'remotesparkmagics'), (u'spark.ui.filters', u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), (u'spark.driver.host', u'wn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net'), (u'spark.yarn.jars', u'local:///usr/hdp/current/spark2-client/jars/*'), (u'spark.executor.id', u'driver'), (u'spark.sql.join.preferSortMergeJoin', u'false'), (u'spark.yarn.appMasterEnv.PYSPARK3_PYTHON', u'/usr/bin/anaconda/envs/py35/bin/python3'), (u'spark.history.fs.logDirectory', u'adl:///hdp/spark2-events'), (u'spark.yarn.dist.archives', u'file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr'), (u'spark.rdd.compress', u'true'), (u'spark.yarn.secondary.jars', u'netty-all-4.0.29.Final.jar,livy-rsc-0.4.0.2.6.5.3003-25.jar,livy-api-0.4.0.2.6.5.3003-25.jar,commons-codec-1.9.jar,livy-core_2.11-0.4.0.2.6.5.3003-25.jar,livy-repl_2.11-0.4.0.2.6.5.3003-25.jar'), (u'spark.submit.pyFiles', u'file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip,file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip'), (u'spark.sql.catalogImplementation', u'hive'), (u'spark.livy.spark_major_version', u'2'), (u'spark.sql.cbo.enabled', u'true'), (u'spark.sql.files.maxPartitionBytes', u'1073741824'), (u'spark.yarn.scheduler.heartbeat.interval-ms', u'5000'), (u'spark.submit.deployMode', u'cluster'), (u'spark.executorEnv.PYTHONPATH', u'/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip'), (u'spark.yarn.submit.waitAppCompletion', u'false'), (u'spark.yarn.appMasterEnv.PYSPARK_PYTHON', u'/usr/bin/anaconda/bin/python'), (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', u'http://hn0-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:8088/proxy/application_1541449485506_0021,http://hn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:8088/proxy/application_1541449485506_0021'), (u'spark.sql.warehouse.dir', u'/hive/warehouse'), (u'spark.yarn.preserve.staging.files', u'false'), (u'spark.sql.crossJoin.enabled', u'true'), (u'spark.extraListeners', u'com.microsoft.hdinsight.spark.metrics.SparkMetricsListener,org.apache.spark.sql.scheduler.EnhancementSparkListener'), (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', u'hn0-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net,hn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net'), (u'spark.yarn.maxAppAttempts', u'1'), (u'spark.executor.instances', u'2'), (u'spark.yarn.historyServer.address', u'hn0-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:18080'), (u'spark.history.provider', u'org.apache.spark.deploy.history.FsHistoryProvider'), (u'spark.yarn.tags', u'livy-session-9-9TvxmFvL'), (u'spark.driver.port', u'40175'), (u'spark.repl.class.uri', u'spark://wn1-sparkd.vpkeqfqajanupfb3sxx3abwftc.jx.internal.cloudapp.net:40175/classes'), (u'spark.eventLog.dir', u'adl:///hdp/spark2-events'), (u'spark.history.ui.port', u'18080'), (u'spark.yarn.app.container.log.dir', u'/mnt/resource/hadoop/yarn/log/application_1541449485506_0021/container_1541449485506_0021_01_000001'), (u'spark.yarn.dist.jars', u'file:///usr/hdp/current/livy2-server/rsc-jars/netty-all-4.0.29.Final.jar,file:///usr/hdp/current/livy2-server/rsc-jars/livy-rsc-0.4.0.2.6.5.3003-25.jar,file:///usr/hdp/current/livy2-server/rsc-jars/livy-api-0.4.0.2.6.5.3003-25.jar,file:///usr/hdp/current/livy2-server/repl_2.11-jars/commons-codec-1.9.jar,file:///usr/hdp/current/livy2-server/repl_2.11-jars/livy-core_2.11-0.4.0.2.6.5.3003-25.jar,file:///usr/hdp/current/livy2-server/repl_2.11-jars/livy-repl_2.11-0.4.0.2.6.5.3003-25.jar'), (u'spark.driver.extraJavaOptions', u'-Dhdp.version= -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC'), (u'spark.yarn.queue', u'default'), (u'spark.driver.memoryOverhead', u'384'), (u'spark.master', u'yarn'), (u'spark.ui.port', u'0'), (u'spark.executor.memory', u'1024m'), (u'spark.yarn.submit.file.replication', u'3'), (u'spark.yarn.access.hadoopFileSystems', u'hdfs://mycluster'), (u'spark.history.kerberos.principal', u'none'), (u'spark.yarn.isPython', u'true'), (u'spark.executor.cores', u'1'), (u'spark.pyspark.python', u'/usr/bin/anaconda/bin/python')]"}], "metadata": {"collapsed": false}}, {"execution_count": 14, "cell_type": "code", "source": "%%configure\n{\"executorMemory\": \"3072M\", \"executorCores\": 4, \"numExecutors\":10}", "outputs": [{"output_type": "stream", "name": "stderr", "text": "'SparkContext' object has no attribute 'get'\nTraceback (most recent call last):\nAttributeError: 'SparkContext' object has no attribute 'get'\n\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}